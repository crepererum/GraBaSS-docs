\chapter{Datasets}
High dimensional datasets are not a very common research topics. In this chapter I will describe some data sources and possible applications of the described algorithm. I also show some interpretations of subspaces in different datasets and some failed approaches of finding them. To get, isolate and preprocess the data I've used some scripts. Because I think open research should not only contain open publications but also open data sources, you can find the scripts and some notes about them online.\footnote{\url{https://github.com/crepererum/GSD}} Should you have some questions, find a bug or want to contribute, feel free to use the issue tracker or file a pull request.

\section{Audio Spectrum}
An audio spectrum can characterize the underlying material. I analyzed the songs from the following collections:
\begin{itemize}
	\item Daft Punk -- Random Access Memories (\circled{P}2013 by Daft Life, Columbia), \num{13} songs
	\item Mayday 2013 -- Never Stop Raving (\circled{C}2013 by Contor Records GmbH), \num{60} songs
\end{itemize}
I converted the audio to a sample rate of \SI{44100}{\hertz}. After it, I splitted the audio files into parts containing \num{4096} parts, applied the Hamming window and passed it to a FFT. The FFT produces a output vector with \num{4096} values, but only \num{2048} values are usable, because the spectrum should be mirrored. The values are normalized and converted to \si{\decibel}. Then I calculated the mean of \num{32} parts to supress noise, which is a common audio effect in modern music, but may also effect classical records. This mean vector has a length of \num{2048} and is written to a file. So, the mean of \num{32} parts is one data element. Figure~\ref{fig:audio} shows some of the results.
\begin{figure}
	\input{figures/audio.tex}
	\caption{Audio spectrum}
	\label{fig:audio}
\end{figure}

The subspace analyzis performance well but gives boring results. Meanly the first and last bins of the spectrum forms one subsapce and the middle part describes another. For this kind of data, time wraping (\cite{timeWraping}) may be the better analyzing approach.

\section{Drug Database}
Another source of high dimensional data is the list of incredients of drugs. A given set of drugs described by a list of their incredients is processed as followed: Build a list of all incredients in all drugs and bring them into a fixed order. Every incredient forms one dimension. Then build a binary vector for every drug in the database where $1$ describes that a incredient is contained in the drug and $0$ if this is not the case.
I hoped to find some public drug data bases containing drugs registered in Germany or in the EU. But this was not the case. Either they were only usable via a very limited interface or it would cost me a enorm amount of money to buy a license. Since the open data movement in USA has a very long tradition, it was easy to find an american drug database, that is easy to parse and provides a huge data collection.\footnote{\url{http://dailymed.nlm.nih.gov/dailymed/downloadLabels.cfm}} I provide a parser and converter for this kind of data.

