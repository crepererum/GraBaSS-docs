\chapter{Designing a high perfomance algorithm}\label{chap:algodesign}
To handle current and future big datasets a special approach is required. In this chapter, I will design a high perfomance algorithm that is able to handle this task. The main goal is a method that does not beat the comlexity of $\mathcal{O} ( \card{D} \cdot \card{N} \cdot \log{\card{N}} + \card{D}^2 \cdot \card{N} )$. This complexity is a good upper bound. It combines the bounds of two different task. The first one is the preprocessing and structure detection of all objects in all dimensions. $\mathcal{O} ( \card{D} \cdot \card{N} )$ would be enough to normalize the data, but $\mathcal{O} ( \card{D} \cdot \card{N} \cdot \log{\card{N}} )$ allowes some better methods like sorting. The second part is the detection of relations between all dimensions, which could be done in $\mathcal{O} ( \card{D}^2 )$. Because reducing the dimenions to a fixed set of features does honor a non fixed number of elements, a good method needs $\mathcal{O} ( \card{D}^2 \cdot \card{N} )$.

\section{A question of similarity}
To search and find subspaces and to use them for further analysis, it is necessary to describe, when dimenions should be share the same subspace. In words, a good but very common describtion is, that they should be similar. But this term has different meanings for different applications and in different fields of science. To use the results for analysis, it is important that you know which structures or attributes dimenions of the same subspace share. It does not make any sense to use a metric based on covariance for the subspace search algorithm, when your outlier detection uses an manhatten metric to detect high distance points. So, the similarity metric should be choosen wisely. To do so, it should defined:

\begin{envdef}[Similarity]
	A similary $s : D \times D \rightarrow [0,1]$ describes, how similar two dimensions are. $0$ means ``not similar'' and $1$ means ``very similar''. It must satisfy the following attributes:
	\begin{enumerate}
		\item Computability: Their has to be an algorithm, two calculate the similary for two given dimenions and a fixed number of points.
		\item Symmetry: For all dimentions $a,b \in D$ it holds that $s(a,b) = s(b,a)$.
		\item Identity: For all dimentions $a \in D$ it holds that $s(a,a) = 1$.
	\end{enumerate}
\end{envdef}

The definition also includes the computability. Because this thesis is about high performance feature selection, the complexity of the algorithm is highly important.\footnote{Please notice, that big constant factors are also important if they are part of the algorithm. Constant factors should be small if possible.} I will no try to evaluate diffent approaches and try to categorize them.

A well known similary is based on the Pearson correlation coefficient. It can be cutted to $[0,1]$, squared or the absolute value can be calcuated to get a similary. The correlation coefficient can be calculated in $\mathcal{O}(\card{N})$. For many applications, this type of similary is sufficient. Because it describes a linear relation, there is an underlying structure which has to exist. Because this structure only has two degrees of freedom, it is very baised.

To handle complex data, a less baised similary has to be constructed. One way to do it is to utilizy metrics:

\begin{envtheo}
	Given a limited metric $m : D \times D \rightarrow [0,\infty)$, $l := \sup_{a,b \in D \times D} m(a,b) < \infty$ a similarity can be constructed in the following way:
	\begin{equation}
		s(a,b) = 1 - \frac{m(a,b)}{l}
	\end{equation}
\end{envtheo}

When the dimenions of the input set are limited, all $p$-norms can be used to construct a similarity by calculating the distance of all points in the dataset:

\begin{envtheo}
	Given a $p$-norm $\|\cdot\|_p$, a similary can be constructed by using the normalized sum of all distances:
	\begin{equation}
		s(a,b) = 1 - \frac{\|\pi_a(N) - \pi_b(N)\|_p}{\card{N}}
	\end{equation}
\end{envtheo}

$p$-norm can be calculated very efficient but are more baised than the Pearson correlation coefficient, because they are only high if two dimenions are nearly equal in many dimenions.\footnote{The number of this dimenions depends on the choosen $p$} So they are also baised and not usable for many applications.

Another approach to create new similarities is to utilizy a metric together with a machine learning algorithm. This should be less baised because many machine learning algorithms can produce very unbaised results:

\begin{envtheo}
	Given a two dimenions $a,b \in D$, a set of prediction algorithms $P = \{ p : \set{R} \rightarrow \set{R} \}$ and a machine learning algorithm $l : (\set{R} \rightarrow \set{R}) \rightarrow P$, a similary can be formed by choosing two subsets $T,V \subseteq N$, $T \cup V = N$, train the preciction algorithm and validating the results by messuring the difference:
	\begin{align}
		p_1 &= l( \pi_a(T), \pi_b(T) )\\
		p_2 &= l( \pi_b(T), \pi_a(T) )\\
		s_1 &= 1 - \frac{\|\pi_b(V) - p_1(\pi_a(N))\|}{\card{V}}\\
		s_2 &= 1 - \frac{\|\pi_a(V) - p_2(\pi_b(N))\|}{\card{V}}\\
		s(a,b) &= s_1 \cdot s_2
	\end{align}
\end{envtheo}

Please choose the training and validation set wisely, especially when you expect outliers in the dataset.

\begin{figure}
	\input{figures/similarities.tex}
	\caption{Diffrent similarities}
\end{figure}

\section{From similarity to subspace}
Based on the combined similarity it is possible to calculate subspaces. It is possible to calculate distances between all pairs of dimensions, because of the nature of subspaces. So it is important to find groups in this $\card{D}^2$ relations highly efficient.

Next, I want to discuss how subspaces are formed. Two dimenions should be contained in the subspaces, when they are similar. This can be expressed by a threshold $p_t \in [0,1]$, which the combined similarity should have. By stripping down the similarity matrix to a binary matrix, the dimenions form a graph. This speeds up the calculation, because it allows fast set operations and memory effective management. Because the most data sets have very different dimensions, the number of edges should be low.

Another important attribute of subspaces is that all contained dimenions are similar to each other. This attribute is already described as clique. Because the graph is dense it has a low degeneracy $d \in \set{N}_0$. So, the cliques can be calucated in $\mathcal{O}(d \cdot (\card{D} - d) \cdot 3^{\frac{d}{3}})$ by using \cite{listingCliques}. Because the cliques forms the subspaces, this is the complete algorithm for converting the combined similarity to subspaces.

\section{Optimize graph structure}
A core problem of strict clique searchers is the heavy fragmentation if only one edge missing. Figure~\ref{subfig:opt_graph1} shows an example where the nodes semi clique $\left\{1,2,3,4\right\}$ is splitted into two cliques because of the absence of the edge $(1,3)$. For some applications it could be helpful to add this edge to the graph and join the two cliques into one. The first method to do it is the utilyzation a distance graph:
\begin{envdef}[Strict Distance Graph]
	Given a graph $G=(V,E)$ and a distance $n \in \set{N}_0$, the strict distance graph $G^n=(V,E^n)$ is give by:
	\begin{align}
		E^0 &= \left\{ (v,v) \in V^2 \right\}\\
		E^i &= \left\{ (v_0,v_2) \in V^2 \mid \exists v_1 \in V : (v_0,v_1) \in E^{i-1}, (v_1,v_2) \in E \right\}, \quad \forall i>0
	\end{align}
\end{envdef}
\begin{figure}
	\caption{Different subspaces from different graph preprocessing}
	\label{fig:opt_graph}
	\centering
	\subfloat[\label{subfig:opt_graph1}Too many subspaces]{\input{figures/graph1.tex}}
	\hfill
	\subfloat[\label{subfig:opt_graph2}Simple graph distance]{\input{figures/graph2.tex}}
	\hfill
	\subfloat[\label{subfig:opt_graph3}Filtered distance with $\alpha=\frac23$]{\input{figures/graph3.tex}}
	\hfill
	\subfloat[\label{subfig:opt_graph4}Bidirectional filtered distance with $\alpha=\frac23$]{\input{figures/graph4.tex}}
\end{figure}
Because it does not make any sense to forget about the given edges when calcuating higher distances, the definition of the joined distance graph ist more useful:
\begin{envdef}[Joined Distance Graph]
	Given a graph $G=(V,E)$ and a distance $n \in \set{N}_+$, the joined distance graph $G^{\overline{n}}=(V,E^{\overline{n}})$ is definied as followed:
	\begin{equation}
		E^{\overline{n}} = \bigcup_{i=1}^n E^i
	\end{equation}
\end{envdef}
Using the joined distance graph fixes the problem, but does not produce the desired result. As seen in Figure~\ref{subfig:opt_graph2}, it attaches to many new vertices to the cliques and make the results unusable. The reason is that it does not count the strenght of the connection in the exisiting graph and so it creates some edges to weakly connected vertices. A messure for the connection rate between two vertices can be calculated by the following equation:
\begin{envdef}[Connection Rate]
	Given a graph $G=(V,E)$ and two vertices $v_1,v_2 \in V$, the connection rate $r_G(v_1,v_2) \in \left[0,1\right]$ can be calcuated by the relation of connected to all neighbors of $v_1$:
	\begin{equation}
		r_G(v_1,v_2) = \frac{
			\card{ \left\{ v \in V \mid (v_1, v) \in E \right\} \cap \left\{ v \in V \mid (v,v_2) \in E \right\} }
		}{
			\card{ \left\{ v \in V \mid (v_1, v) \in E \right\} }
		}
	\end{equation}
\end{envdef}
Please notice that the connection rate is not symmetric. To honor the connection rate of the vertices, I use a filtered strict distance graph:
\begin{envdef}[Filtered Strict Distance Graph]
	Given a graph $G=(V,E)$, a distance $n \in \set{N}_+$ and a filter rate $\alpha \in \left[0,1\right]$, the filtered strict distance graph $G_\alpha^n=(V,E_\alpha^n)$ is definined as followed:
	\begin{equation}
		E_\alpha^n = \left\{ (v_1,v_2) \in E^n \mid r_{G^{n-1}}(v_1,v_2) \geq \alpha \right\}
	\end{equation}
\end{envdef}
The joined filtered graph, that considers all distances up to a given value, is definied analog:
\begin{envdef}[Filtered Joined Distance Graph]
	Given a graph $G=(V,E)$a distance $n \in \set{N}_+$ and a filter rate $\alpha \in \left[0,1\right]$, the filtered joined distance graph $G_\alpha^{\overline{n}}=(V,E_\alpha^{\overline{n}})$ is definied as followed:
	\begin{equation}
		E_\alpha^{\overline{n}} = \bigcup_{i=1}^n E_\alpha^i
	\end{equation}
\end{envdef}
The filtered joined distance graph is not bidirectional. This leads the problem that there are unidirectional edges that are not usable for our result. As Figure~\ref{subfig:opt_graph3} shows, the weakly connected nodes are connected to the semiclique whereas the semiclique is not connected to the satallites. The inter semiclique edges are bidirectional. Based on this fact, a bidirectional graph can be constructed:
\begin{envdef}[Bidirectional Joined Filtered Distance Graph]
	Given a graph $G=(V,E)$, a distance $n \in \set{N}_+$ and a filter rate $\alpha \in \left[0,1\right]$, the bidirectional joined filtered graph $\tilde{G}_\alpha^{\overline{n}}=(V,\tilde{E}_\alpha^{\overline{n}})$ is defined by the subset of the bidirectional edges of the joined filtered graph:
	\begin{equation}
		\tilde{E}_\alpha^{\overline{n}} = \left\{ (v_1,v_2) \in V^2 \mid (v_1,v_2),(v_2,v_1) \in E_\alpha^{\overline{n}} \right\}
	\end{equation}
\end{envdef}
As shown on Figure~\ref{subfig:opt_graph4} this method successfully adds inter semiclique edges to join them to cliques without creating senseless connections or resulting in informaion loosage. It is important that the graph refinement does not slow down the entire process on big data. In other words, it must not have a higher complexity class. Because the distance parameter $n$ is only used for graph fixing and should not depend on the input size, the following theorem can be shown:

\begin{algorithm}
	\KwData{Vertex $v$}
	\KwData{Graph $G$}
	\KwData{Threshold $t$}
	\KwResult{New neighbors $N$}

	\SetKwFunction{CalcConnectionRate}{calcConnectionRate}
	\SetKwFunction{GetNeighbors}{getNeighbors}

	\Begin{
		\tcc{Search all candidates}
		$C$ $\leftarrow$ $\{\}$\;
		\For{$w \in \GetNeighbors{$v$}$}{
			$C$ $\leftarrow$ $C \cup \GetNeighbors{$w$}$\;
		}

		\BlankLine
		\tcc{Check all candidates}
		$N$ $\leftarrow$ $\{\}$\;
		\For{$c \in C$}{
			\If{$\CalcConnectionRate{$v$, $c$} \geq t \wedge \CalcConnectionRate{$c$, $v$} \geq t$}{
				$N$ $\leftarrow$ $N \cup \{c\}$\;
			}
		}
	}
	\caption{extendNeighbors}
\end{algorithm}

\begin{algorithm}
	\KwData{Graph $G$}
	\KwData{Distance $d$}
	\KwData{Threshold $t$}
	\KwResult{Refined Graph $G_r$}

	\SetKwFunction{ExtendNeighbors}{extendNeighbors}

	\Begin{
		$G_r$ $\leftarrow$ $G$\;
		\For{i=2 \emph{\KwTo} $d$}{
			$G_n$ $\leftarrow$ $()$\;
			\For{$v \in G_r$}{
				$G_n$ $\leftarrow$ $G_n \concat \ExtendNeighbors{$v$, $G_r$, $t$}$\;
			}
			$G_r$ $\leftarrow$ $G_n$\;
		}
	}
	\caption{refineGraph}
\end{algorithm}

\begin{envtheo}
	The graph refinement can be done in $\mathcal{O} ( \card{V}^2 )$ when choosing a fixed distance parameter $n \in \set{N}_+$.
\end{envtheo}
\begin{proof}
	To graph will generated from the graph with the distance $n-1$ where a distance of $1$ is the input graph. Every of this rounds contains two steps. Step one is the calculation of the connection rates, which can be done in $\mathcal{O} ( \card{V}^2 )$. This assumes that data structures are used that can answert the question ``Is vertix $v_1$ connceted to $v_2$?'' in $\mathcal{O} ( 1 )$. The second calculation step is the construction of the new neighborhood for every node. Assuming that you use $\mathcal{O} ( 1 )$ set operations, this can be done for all vertices in $\mathcal{O} ( \card{V}^2 )$. This results in a total cost per round of $\mathcal{O} ( \card{V}^2 )$ and a total algorithm cost of $\mathcal{O} ( n \cdot \card{V}^2 ) = \mathcal{O} ( \card{V}^2 )$.
\end{proof}

\section{Algorithm Summary}
\begin{algorithm}
	\KwData{Dimension $d$}
	\KwData{Binsize $s$}
	\KwResult{Bins $B$}

	\SetKwData{Undef}{undef}

	\SetKwFunction{Sort}{sort}

	\Begin{
		$B$ $\leftarrow$ $()$\;
		$d_s$ $\leftarrow$ \Sort{$d$}\;
		$x$ $\leftarrow$ \Undef\;
		$b$ $\leftarrow$ $()$\;
		\For{$x \in d_s$}{
			\If{$|B| \geq s \wedge x \neq l$}{
				$B$ $\leftarrow$ $B \concat b$\;
				$b$ $\leftarrow$ $()$\;
			}
			$b$ $\leftarrow$ $B \concat x$\;
			$l$ $\leftarrow$ $x$\;
		}
		\If{$|b| \neq 0$}{
			$B$ $\leftarrow$ $B \concat b$\;
		}
	}
	\caption{buildBins}
\end{algorithm}

\begin{algorithm}
	\KwData{Dataset $D$}
	\KwData{Threshold $p_t$}
	\KwData{Graph distance $p_d$}
	\KwData{Threshold of the graph connection rate $p_c$}
	\KwResult{Subspaces $S$}

	\SetKwFunction{BuildBins}{buildBins}
	\SetKwFunction{CalcMeanValues}{calcMeanValues}
	\SetKwFunction{CalcStddevValues}{calcStddevValues}
	\SetKwFunction{CalcVarValues}{calcVarValues}
	\SetKwFunction{CoVar}{coVar}
	\SetKwFunction{DimSimilarity}{dimSimilarity}
	\SetKwFunction{RefineGraph}{refineGraph}
	\SetKwFunction{SearchCliques}{searchCliques}

	\Begin{
		\tcc{Pre calculate aggregates}
		$D_m$ $\leftarrow$ \CalcMeanValues{$D$}\;
		$D_v$ $\leftarrow$ \CalcVarValues{$D$, $D_m$}\;
		$D_s$ $\leftarrow$ \CalcStddevValues{$D_v$}\;

		\BlankLine
		\tcc{Build grid}
		$D_g$ $\leftarrow$ $()$\;
		\For{$d \in D$}{
			$D_g$ $\leftarrow$ $D_g \concat$ \BuildBins{$d$, $\sqrt{|D|}$}\;
		}

		\BlankLine
		\tcc{Build graph}
		\For{$(d_1,m_1,s_1,g_1) \in (D,D_m,D_s,D_g)$}{
			$n$ $\leftarrow$ $\{\}$\;
			\For{$(d_2,m_2,s_2,g_2) \in (D,D_m,D_s,D_g)\setminus\{(d_1,m_1,s_1,g_1)\}$}{
				$x_1$ $\leftarrow$ $\frac{\CoVar{$d_1$, $d_2$, $m_1$, $m_2$}}{s_1 \cdot s_2}$\;
				$x_2$ $\leftarrow$ \DimSimilarity{$g_1$, $g_2$}\;
				\If{$x_1 \cdot x_2 \geq p_t$}{
					$n$ $\leftarrow$ $n \cup \{d_2\}$\;
				}
			}
			$G$ $\leftarrow$ $G \concat n$\;
		}

		\BlankLine
		\tcc{Refine graph}
		$G_r$ $\leftarrow$ \RefineGraph{$G$, $p_d$, $p_c$}\;

		\BlankLine
		\tcc{Search cliques (which are our subspaces)}
		$S$ $\leftarrow$ \SearchCliques{$G_r$}\;
	}
	\caption{calcSubspaces}
\end{algorithm}

