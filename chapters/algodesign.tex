\chapter{Designing a high performance algorithm}\label{chap:algodesign}
To handle current and future big datasets a special approach is required. In this chapter I will design a high performance algorithm that is able to handle this task. The main goal is a method that does not beat the complexity of $\mathcal{O} ( \card{D} \cdot \card{N} \cdot \log{\card{N}} + \card{D}^2 \cdot \card{N} )$. This complexity is a good upper bound. It combines the bounds of two different tasks. The first one is the preprocessing and structure detection of all objects in all dimensions. $\mathcal{O} ( \card{D} \cdot \card{N} )$ would be enough to normalize the data, but $\mathcal{O} ( \card{D} \cdot \card{N} \cdot \log{\card{N}} )$ allows some better methods like sorting. The second part is the detection of relations between all dimensions, which could be done in $\mathcal{O} ( \card{D}^2 )$. In combination with a non-fixed number a elements, a good method needs $\mathcal{O} ( \card{D}^2 \cdot \card{N} )$.

To define a algorithm, it is necessary to describe what it should calculate. As stated in the introduction it should extract subspaces from the data set, so a description of what a subspace is would be helpful:
\begin{envdef}[Subspace]
	A subspace is a collection of dimensions which are similar to each other. This collection should be maximal. It is possible that one dimension is contained in multiple subspaces or, in other words, that subspaces overlap.
\end{envdef}

According to the definitions the set of all subspaces does to not partition the set of dimensions. The definition also avoids a description if the relation of dimensions in one subspace are pairwise or can only be described by functions with more than two arguments. For GraBaSS I chose a pairwise definition because it is sufficient for the most real world data sets. As tests in chapter~\ref{chap:datasets} show, I am right with this assumption. Chapter~\ref{chap:beat} describes when this can be a problem and how to work around it.

\section{A question of similarity}
To search and find subspaces and to use them for further analysis, it is necessary to describe when dimensions should be share the same subspace. In words, a good but very common description is that they should be similar. But this term has different meanings for different applications and in different fields of science. To use the results for analysis, it is important to know which structures or attributes dimensions of the same subspace share. It does not make any sense to use a metric based on covariance for the subspace search algorithm when your outlier detection uses an Manhattan metric to detect high distance points. So, the similarity metric should be chosen wisely. To do so, it should defined:

\begin{envdef}[Similarity]
	A similarity $s : D \times D \rightarrow [0,1]$ describes how similar two dimensions are. $0$ means ``not similar'' and $1$ means ``very similar''. It must satisfy the following attributes:
	\begin{enumerate}
		\item Computability: Their has to be an algorithm, two calculate the similarity for two given dimensions and a fixed number of points.
		\item Symmetry: For all dimensions $a,b \in D$ it holds that $s(a,b) = s(b,a)$.
		\item Identity: For all dimensions $a \in D$ it holds that $s(a,a) = 1$.
	\end{enumerate}
\end{envdef}

The definition also includes the computability. Because this thesis is about high performance feature selection, the complexity of the algorithm is highly important.\footnote{Please notice that big constant factors are also important if they are part of the algorithm. Constant factors should be small if possible.}

A well known similarity is based on the Pearson correlation coefficient. It can be cut to $[0,1]$, squared or the absolute value can be calculated to get a similarity. The correlation coefficient can be calculated in $\mathcal{O}(\card{N})$. For many applications, this type of similarity is sufficient. Because it describes a linear relation, there is an underlying structure which has to exist. Because this structure only has two degrees of freedom, it is very biased.

To handle complex data, a less biased method has to be constructed. One way to do it is to utilize metrics:

\begin{envtheo}
	Given a limited metric $m : D \times D \rightarrow [0,\infty)$, $l := \sup_{a,b \in D \times D} m(a,b) < \infty$ a similarity can be constructed in the following way:
	\begin{equation}
		s(a,b) = 1 - \frac{m(a,b)}{l}
	\end{equation}
\end{envtheo}

When the dimensions of the input set are limited, all $p$-norms can be used to construct a similarity by calculating the distance of all points in the dataset:

\begin{envtheo}
	Given a $p$-norm $\|\cdot\|_p$, a similarity can be constructed by using the normalized sum of all distances:
	\begin{equation}
		s(a,b) = 1 - \frac{\|\pi_a(N) - \pi_b(N)\|_p}{\card{N}}
	\end{equation}
\end{envtheo}

$p$-norms can be calculated very efficient but are more biased than the Pearson correlation coefficient, because they are only high if two dimensions are nearly equal in many dimensions.\footnote{The number of this dimensions depends on the chosen $p$} So they are also biased and not usable for many applications.

Another approach to create new similarities is to combine a metric together with a machine learning algorithm. This should be less biased because many machine learning algorithms can produce very flexible results:

\begin{envtheo}
	Given a two dimensions $a,b \in D$, a set of prediction algorithms $P = \{ p : \set{R} \rightarrow \set{R} \}$ and a machine learning algorithm $l : (\set{R} \rightarrow \set{R}) \rightarrow P$, a similarity can be formed by choosing two subsets $T,V \subseteq N$, $T \cup V = N$, train the prediction algorithm and validating the results by measuring the difference:
	\begin{align}
		p_1 &= l( \pi_a(T), \pi_b(T) )\\
		p_2 &= l( \pi_b(T), \pi_a(T) )\\
		s_1 &= 1 - \frac{\|\pi_b(V) - p_1(\pi_a(N))\|}{\card{V}}\\
		s_2 &= 1 - \frac{\|\pi_a(V) - p_2(\pi_b(N))\|}{\card{V}}\\
		s(a,b) &= s_1 \cdot s_2
	\end{align}
\end{envtheo}

Please choose the training and validation set wisely, especially when you expect outliers in the dataset.

Since machine learning algorithms allow comparison of dimensions with flexible structure, an important question is how flexible a structure can be to allow a comparison. Mathematics had found a answer to this question long time ago. They defined the term and rules of independence:

\begin{envdef}[Independence]
	Two dimensions $X,Y \in D$ are independent, iff:
	\begin{equation}
		\prob{X=z,Y=z} = \prob{X=z}\prob{Y=z},\quad \forall z \in \set{R}
	\end{equation}
\end{envdef}

This definition is used by many mathematics worldwide but has one problem. If we assume a limited dataset with real number values and just a little bit random noise, no value in the dimensions will appear twice. According to the definition of independence, all dimensions will be independent in this situation. So it is necessary to extend the Dirac masses to another probability. This can be done by using window functions, which leads to an continuous probability. Another method is binning by using fixed size bins. Because it is possible that the data resolution is different in different parts of the dimension, guessing window sizes or bin widths is not possible every time and can lead to wrong results. To circumvent this problem, I will introduce a method that is known widely but not used by many researcher and analysts: binning with fixed number of objects per bin. To express the effect of this method, I will call it rewrapping.

Rewrapping handles dense data regions very well and ignores non used regions. It is also able to reverse every strict monotone function. This kind of function may occur as part of the measurement of real world data. Figure~\ref{fig:rewrap} illustrates the process of rewrapping. The number of bins is not a user provided parameter. It is set to $\sqrt{\card{N}}$ to honor the fact the more data points also describe a more detailed information about the distribution.

\begin{figure}
	\caption{Rewrapping}
	\label{fig:rewrap}
	\centering
	\subfloat[\label{subfig:rewrap1_source}Sample 1: Input data]{\input{figures/rewrap1_source.tex}}
	\hfill
	\subfloat[\label{subfig:rewrap1_target}Sample 1: Result of rewrapping]{\input{figures/rewrap1_target.tex}}
	\\
	\subfloat[\label{subfig:rewrap2_source}Sample 2: Input data]{\input{figures/rewrap2_source.tex}}
	\hfill
	\subfloat[\label{subfig:rewrap2_target}Sample 2: Result of rewrapping]{\input{figures/rewrap2_target.tex}}
	\\
	\subfloat[\label{subfig:rewrap3_source}Sample 3: Input data]{\input{figures/rewrap3_source.tex}}
	\hfill
	\subfloat[\label{subfig:rewrap3_target}Sample 3: Result of rewrapping]{\input{figures/rewrap3_target.tex}}
\end{figure}

Because real value data can contain discrete values, a small modification is required. Bins are created from the lower to the upper bound of the dataset. If the border between a created and a new bin will split a set of equal values, the entire set will assigned to the lower bin. This can lead to a different bin number and fill size. See algorithm~\ref{alg:buildBins} for a complete definition.

\begin{algorithm}
	\KwData{Dimension $d$}
	\KwData{Binsize $s$}
	\KwResult{Bins $B$}

	\SetKwData{Undef}{undef}

	\SetKwFunction{Sort}{sort}

	\Begin{
		$B$ $\leftarrow$ $()$\;
		$d_s$ $\leftarrow$ \Sort{$d$}\;
		$x$ $\leftarrow$ \Undef\;
		$b$ $\leftarrow$ $()$\;
		\For{$x \in d_s$}{
			\If{$|B| \geq s \wedge x \neq l$}{
				$B$ $\leftarrow$ $B \concat b$\;
				$b$ $\leftarrow$ $()$\;
			}
			$b$ $\leftarrow$ $B \concat x$\;
			$l$ $\leftarrow$ $x$\;
		}
		\If{$|b| \neq 0$}{
			$B$ $\leftarrow$ $B \concat b$\;
		}
	}
	\caption{buildBins}
	\label{alg:buildBins}
\end{algorithm}

To extract a measure of independence from the discrete data set, the mutual information is calculated:

\begin{envdef}[Mutual Information]
	For two discrete dimensions $X$ and $Y$, which are only described by their bins and the probability of each bin, the mutual information $\mutual{X}{Y}$ is calculated by:
	\begin{equation}
		\mutual{X}{Y} = \entropy{X} + \entropy{y} - \entropy{X,Y}
	\end{equation}
	To do this, the entropy has to be calculated as followed:
	\begin{align}
		\entropy{X} &= - \sum_{x \in X} p(x) \log{p(x)}\\
		\entropy{X,Y} &= - \sum_{x \in X, y \in Y} p(x,y) \log{p(x,y)}
	\end{align}
\end{envdef}

To convert the value of mutual information to a similarity, it has to be normalized by the following method:

\begin{envtheo}[Normalized Mutual Information]
	The normalized mutual information of two dimension $X$ and $Y$ is calculated by the following equation
	\begin{equation}
		\mutual{x}{y}_{norm} = \frac{\mutual{X}{Y}}{\min{\left( \entropy{X}, \entropy{Y} \right)}}
	\end{equation}
\end{envtheo}
\begin{proof}
	See \cite{mutual}.
\end{proof}

Mutual information and independence do not respect permutation of the bins. This can lead to very unintuitive results and makes the calculation heavily depend on the number of bins and their borders. Figure~\ref{fig:permutation} shows an example of this problem. The dimensions of the two shown data sets have the same value of mutual information, but when it is clear that the dimensions shown in figure~\ref{subfig:perm_good} are more similar as the dimensions shown in figure~\ref{subfig:perm_bad}. To fix this, the information from the absolute Pearson correlation coefficient could be mixed in. This describes if two dimensions are linear correlated and avoids random permutations. The two similarities should be combined in a way that acts like a logic AND. Because both values are real, but have the same range, a limited, non-complete logic can be used:

\begin{figure}
	\caption{Permutation of bins}
	\label{fig:permutation}
	\centering
	\subfloat[\label{subfig:perm_good}Permutation with high similarity]{\input{figures/permutation_good.tex}}
	\hfill
	\subfloat[\label{subfig:perm_bad}Permutation with low similarity]{\input{figures/permutation_bad.tex}}
\end{figure}

\begin{envdef}[Similarity Logic]
	Similarities can be used as logical values in the following way:
	\begin{align}
		true &\equiv 1\\
		false &\equiv 0\\
		\neg a &\equiv 1 - a\\
		a \wedge b &\equiv a \cdot b\\
		a \vee b &\equiv 1 - (1 - a) \cdot (1 - b)
	\end{align}
\end{envdef}

Please note, that this system is not correct in terms of logic, because $a \wedge a = a$, but $a \wedge a \equiv a^2 \neq a$. This fact does not make this system wrong, because it makes sense that the claim of $a \wedge a$ requires $a$ to be stronger than simple claim $a$ alone.

It is now possible to combine different similarities to one final value. As described above, the mutual information is combined with the absolute Pearson correlation coefficient using the AND function or, in other words, by calculating the product of them. Figure~\ref{fig:similarities} gives an overview about the different similarities and how they work together. There are also stronger and weaker definitions in terms of how the rely on a predefined structure. For example the absolute Pearson correlation coefficient only works well with linear correlation but avoids random permutations, whereas the normalized mutual information is more flexible but ignores permutations. Using this building blocks, it is possible to swap the final similarity when doing further research or combine it with new definitions.

\begin{figure}
	\centering
	\input{figures/similarities.tex}
	\caption{Different similarities}
	\label{fig:similarities}
\end{figure}

\section{From similarity to subspace}
Based on the combined similarity it is possible to calculate subspaces by calculating the binary relation between all dimensions and find groups that have strong connections. It is important to find this groups in this $\card{D}^2$ relations highly efficient.

Next, I want to discuss how subspaces are formed. Two dimensions should be contained in the subspaces, when they are similar. This can be expressed by a threshold $p_e \in [0,1]$, which the combined similarity should have. By stripping down the similarity matrix to a binary matrix, the dimensions form a graph. This speeds up the calculation, because it allows fast set operations and memory effective management. Because the most data sets have very different dimensions, the number of edges should be low.

The important attribute of subspaces is that all contained dimensions are similar to each other. For graphs this attribute is already described as clique. Because the graph is dense it has a low degeneracy $d \in \set{N}_0$. So, the cliques can be calculated in $\mathcal{O}(d \cdot (\card{D} - d) \cdot 3^{\frac{d}{3}})$ by using \cite{listingCliques}. Because the cliques forms the subspaces, this is the complete algorithm for converting the combined similarity to subspaces.

\section{Optimizing the graph structure}
A core problem of strict clique searchers is the heavy fragmentation if only one edge missing. Figure~\ref{subfig:opt_graph1} shows an example where the nodes semi-clique $\left\{1,2,3,4\right\}$ is split into two cliques because of the absence of the edge $(1,3)$. For some applications it could be helpful to add this edge to the graph and join the two cliques into one. The first method to do it is the utilization a distance graph:
\begin{envdef}[Strict Distance Graph]
	Given a graph $G=(V,E)$ and a distance $n \in \set{N}_0$, the strict distance graph $G^n=(V,E^n)$ is give by:
	\begin{align}
		E^0 &= \left\{ (v,v) \in V^2 \right\}\\
		E^i &= \left\{ (v_0,v_2) \in V^2 \mid \exists v_1 \in V : (v_0,v_1) \in E^{i-1}, (v_1,v_2) \in E \right\}, \quad \forall i>0
	\end{align}
\end{envdef}
\begin{figure}
	\caption{Different subspaces from different graph preprocessing}
	\label{fig:opt_graph}
	\centering
	\subfloat[\label{subfig:opt_graph1}Too many subspaces]{\input{figures/graph1.tex}}
	\hfill
	\subfloat[\label{subfig:opt_graph2}Simple graph distance]{\input{figures/graph2.tex}}
	\hfill
	\subfloat[\label{subfig:opt_graph3}Filtered distance with $\alpha=\frac23$]{\input{figures/graph3.tex}}
	\hfill
	\subfloat[\label{subfig:opt_graph4}Bidirectional filtered distance with $\alpha=\frac23$]{\input{figures/graph4.tex}}
\end{figure}
Because it does not make any sense to forget about the given edges when calculating higher distances, the definition of the joined distance graph is more useful:
\begin{envdef}[Joined Distance Graph]
	Given a graph $G=(V,E)$ and a distance $n \in \set{N}_+$, the joined distance graph $G^{\overline{n}}=(V,E^{\overline{n}})$ is defined as followed:
	\begin{equation}
		E^{\overline{n}} = \bigcup_{i=1}^n E^i
	\end{equation}
\end{envdef}
Using the joined distance graph fixes the problem but does not produce the desired result. As seen in Figure~\ref{subfig:opt_graph2}, it attaches to many new vertexes to the cliques and make the results unusable. The reason is that it does not count the strength of the connection in the existing graph and so it creates some edges to weakly connected vertexes. A measure for the connection rate between two vertexes can be calculated by the following equation:
\begin{envdef}[Connection Rate]
	Given a graph $G=(V,E)$ and two vertexes $v_1,v_2 \in V$, the connection rate $r_G(v_1,v_2) \in \left[0,1\right]$ can be calculated by the relation of connected to all neighbors of $v_1$:
	\begin{equation}
		r_G(v_1,v_2) = \frac{
			\card{ \left\{ v \in V \mid (v_1, v) \in E \right\} \cap \left\{ v \in V \mid (v,v_2) \in E \right\} }
		}{
			\card{ \left\{ v \in V \mid (v_1, v) \in E \right\} }
		}
	\end{equation}
\end{envdef}
Please notice that the connection rate is not symmetric. To honor the connection rate of the vertexes, I use a filtered strict distance graph:
\begin{envdef}[Filtered Strict Distance Graph]
	Given a graph $G=(V,E)$, a distance $n \in \set{N}_+$ and a filter rate $\alpha \in \left[0,1\right]$, the filtered strict distance graph $G_\alpha^n=(V,E_\alpha^n)$ is definined as followed:
	\begin{equation}
		E_\alpha^n = \left\{ (v_1,v_2) \in E^n \mid r_{G^{n-1}}(v_1,v_2) \geq \alpha \right\}
	\end{equation}
\end{envdef}
The joined filtered graph, that considers all distances up to a given value, is defined analog:
\begin{envdef}[Filtered Joined Distance Graph]
	Given a graph $G=(V,E)$a distance $n \in \set{N}_+$ and a filter rate $\alpha \in \left[0,1\right]$, the filtered joined distance graph $G_\alpha^{\overline{n}}=(V,E_\alpha^{\overline{n}})$ is defined as followed:
	\begin{equation}
		E_\alpha^{\overline{n}} = \bigcup_{i=1}^n E_\alpha^i
	\end{equation}
\end{envdef}
The filtered joined distance graph is not bidirectional. This leads the problem that there are unidirectional edges that are not usable for our result. As Figure~\ref{subfig:opt_graph3} shows, the weakly connected nodes are connected to the semi-clique whereas the semi-clique is not connected to the satellites. The inter semi-clique edges are bidirectional. Based on this fact, a bidirectional graph can be constructed:
\begin{envdef}[Bidirectional Joined Filtered Distance Graph]
	Given a graph $G=(V,E)$, a distance $n \in \set{N}_+$ and a filter rate $\alpha \in \left[0,1\right]$, the bidirectional joined filtered graph $\tilde{G}_\alpha^{\overline{n}}=(V,\tilde{E}_\alpha^{\overline{n}})$ is defined by the subset of the bidirectional edges of the joined filtered graph:
	\begin{equation}
		\tilde{E}_\alpha^{\overline{n}} = \left\{ (v_1,v_2) \in V^2 \mid (v_1,v_2),(v_2,v_1) \in E_\alpha^{\overline{n}} \right\}
	\end{equation}
\end{envdef}
As shown on Figure~\ref{subfig:opt_graph4} this method successfully adds inter semi-clique edges to join them to cliques without creating senseless connections or resulting in information lossage. It is important that the graph refinement does not slow down the entire process on big data. In other words, it must not have a higher complexity class. Because the distance parameter $n$ is only used for graph fixing and should not depend on the input size, the following theorem can be shown:

\begin{algorithm}
	\KwData{Vertex $v$}
	\KwData{Graph $G$}
	\KwData{Threshold $t_n$}
	\KwResult{New neighbors $N$}

	\SetKwFunction{CalcConnectionRate}{calcConnectionRate}
	\SetKwFunction{GetNeighbors}{getNeighbors}

	\Begin{
		\tcc{Search all candidates}
		$C$ $\leftarrow$ $\{\}$\;
		\For{$w \in \GetNeighbors{$v$}$}{
			$C$ $\leftarrow$ $C \cup \GetNeighbors{$w$}$\;
		}

		\BlankLine
		\tcc{Check all candidates}
		$N$ $\leftarrow$ $\{\}$\;
		\For{$c \in C$}{
			\If{$\CalcConnectionRate{$v$, $c$} \geq t_n \wedge \CalcConnectionRate{$c$, $v$} \geq t_n$}{
				$N$ $\leftarrow$ $N \cup \{c\}$\;
			}
		}
	}
	\caption{extendNeighbors}
\end{algorithm}

\begin{algorithm}
	\KwData{Graph $G$}
	\KwData{Distance $d$}
	\KwData{Threshold $t_n$}
	\KwResult{Refined Graph $G_r$}

	\SetKwFunction{ExtendNeighbors}{extendNeighbors}

	\Begin{
		$G_r$ $\leftarrow$ $G$\;
		\For{i=2 \emph{\KwTo} $d$}{
			$G_n$ $\leftarrow$ $()$\;
			\For{$v \in G_r$}{
				$G_n$ $\leftarrow$ $G_n \concat \ExtendNeighbors{$v$, $G_r$, $t_n$}$\;
			}
			$G_r$ $\leftarrow$ $G_n$\;
		}
	}
	\caption{refineGraph}
\end{algorithm}

\begin{envtheo}\label{theo:fixing}
	The graph refinement can be done in $\mathcal{O} ( n \cdot \card{V} \cdot m^2 )$ where $n \in \set{N}_+$ is the fixed distance parameter and $m \in \set{N}_0$ is the number of maximum neighbors over all vertexes in the resulting graph.
\end{envtheo}
\begin{proof}
	In this proof, I assume that the neighbors for of each vertex are stored in a set, which allows lookup and insert operations in $\mathcal{O} (1)$.

	The graph will generated from the graph with the distance $n-1$ where a distance of $1$ is the input graph. Every of this rounds contains three steps and is done for every vertex in the graph. The first step is the listing of all neighbor candidates without taking connection rates into account and joining them into one set. For each vertex, this takes $\mathcal{O} ( m^2 )$ operations and results in $\mathcal{O} ( m )$ candidates. Step two is the calculation of the connectivity, which can be done in $\mathcal{O} ( m )$ for every candidate. For all candidates, it takes $\mathcal{O} ( m^2 )$ operations. The last step is the filtering according to the connectivity and the build-up of the new neighborhood-set, which takes $\mathcal{O} ( m )$ operations. All steps together take $\mathcal{O} ( m^2 )$ operations for each vertex, which is $\mathcal{O} ( \card{V} \cdot m^2)$ in total. Because this process is repeated $n$ times, the total complexity is $\mathcal{O} ( n \cdot \card{V} \cdot m^2 )$.
\end{proof}

Assuming, that the number of maximum neighbors is small and especially lower than $\card{D}$, the graph fixing process does not have a performance impact. Tests prove this assumption, see section~\ref{sec:time} for details.

\section{Algorithm Summary}
After discussing all parts of the algorithm, I will now wrap up the entire workflow and explain some optimizations. The main method is shown in algorithm~\ref{alg:calcSubspaces}. It uses the methods explained earlier. Please note that no method uses a global shared state. This functional attribute helps when it comes to parallelization of the algorithm.

\begin{algorithm}
	\KwData{Dataset $D$}
	\KwData{Threshold $t_e$}
	\KwData{Graph distance $d$}
	\KwData{Threshold of the graph connection rate $t_n$}
	\KwResult{Subspaces $S$}

	\SetKwFunction{BuildBins}{buildBins}
	\SetKwFunction{CalcMeanValues}{calcMeanValues}
	\SetKwFunction{CalcStddevValues}{calcStddevValues}
	\SetKwFunction{CalcVarValues}{calcVarValues}
	\SetKwFunction{CoVar}{coVar}
	\SetKwFunction{DimSimilarity}{dimSimilarity}
	\SetKwFunction{RefineGraph}{refineGraph}
	\SetKwFunction{SearchCliques}{searchCliques}

	\Begin{
		\tcc{Pre calculate aggregates}
		$D_m$ $\leftarrow$ \CalcMeanValues{$D$}\;
		$D_v$ $\leftarrow$ \CalcVarValues{$D$, $D_m$}\;
		$D_s$ $\leftarrow$ \CalcStddevValues{$D_v$}\;

		\BlankLine
		\tcc{Build grid}
		$D_g$ $\leftarrow$ $()$\;
		\For{$d \in D$}{
			$D_g$ $\leftarrow$ $D_g \concat$ \BuildBins{$d$, $\sqrt{|D|}$}\;
		}

		\BlankLine
		\tcc{Build graph}
		\For{$(d_1,m_1,s_1,g_1) \in (D,D_m,D_s,D_g)$}{
			$n$ $\leftarrow$ $\{\}$\;
			\For{$(d_2,m_2,s_2,g_2) \in (D,D_m,D_s,D_g)\setminus\{(d_1,m_1,s_1,g_1)\}$}{
				$x_1$ $\leftarrow$ $\frac{\CoVar{$d_1$, $d_2$, $m_1$, $m_2$}}{s_1 \cdot s_2}$\;
				$x_2$ $\leftarrow$ \DimSimilarity{$g_1$, $g_2$}\;
				\If{$x_1 \cdot x_2 \geq t_e$}{
					$n$ $\leftarrow$ $n \cup \{d_2\}$\;
				}
			}
			$G$ $\leftarrow$ $G \concat n$\;
		}

		\BlankLine
		\tcc{Refine graph}
		$G_r$ $\leftarrow$ \RefineGraph{$G$, $d$, $t_n$}\;

		\BlankLine
		\tcc{Search cliques (which are our subspaces)}
		$S$ $\leftarrow$ \SearchCliques{$G_r$}\;
	}
	\caption{calcSubspaces}
	\label{alg:calcSubspaces}
\end{algorithm}

First of all, some values that only depend on one dimension are pre-calculated, this are the mean, the variance and the standard deviation of the dimensions. This step can parallelized at two points. Because the result for one dimension only depends on this dimension itself, the values for multiple dimensions can be calculated independently. The mean and the variance are sums. This sum up can be parallelized can by multiple methods, because all sub sums are independent.

After this, the discrete bins are created. Because bins are only build on top of the information of one dimension, multiple dimensions can handled in parallel too. The creation of the bins requires that the data is sorted by the actual dimension. This sorting can also be done by a parallel algorithm, e.g. merge sort.

Now, the initial graph is build. This is done by calculating the absolute Pearson covariance coefficient and the normalized mutual information. Then, the product is calculated and an graph edge between this two dimensions is created, if this value is at least $t_e$. I suggest to parallelize the inner loop and leaf the outer one as it is. Depending on the size of the data set, this minimizes the number of page-in and page-out operations.\footnote{Do not hope, that an entire dimension can be hold by the cache, so cache locality is not an argument.} Because the Pearson covariance coefficient and the value of mutual information are symmetric, it is only required to calculated the one half of the adjacency matrix.

Before calculating the cliques, the graph is refined by adding new edges. The performance trick by using the symmetry of the adjacency matrix works here too and the loops can be parallelized in the same way as done in the graph generation process.

Finally, the cliques are searched by \cite{listingCliques}. Because the algorithm has multiple loops with independent recursions in them, it can be parallelized at this points.

\section{Choosing parameters}
Choosing the right parameters is important to get usable results. This choice also depends on the problem you want to solve or the information you want to extract from the data. The usage of the data by automatic analysis methods targets high information volume and low error rates. Manual, human driven analysis and exploration requires short, readable and non-redundant reports. In this case, a higher error rate is accepted to simplify this task. The GraBaSS has \num{3} parameters, $t_e$, $t_n$ and $d$ which should be used as followed.

The first parameter is $t_e \in [0,1]$, which is the threshold that describes the lower bound of the similarity of an edge of the graph. If the similarity of two dimensions is at least $t_e$, they are assumed to be similar. This results in an edge in the graph structure and is used by the clique searcher. I suggest to choose \num{0.4} as start parameter. If the algorithm finds too small subspaces, decrease the parameter. If the results contains only a few but big subspaces, the parameter is too low.

The second parameter $t_n \in [0,1]$ and the third parameter $d \in [0,\infty]$ should be used together. $d$ describes the maximum graph distance for the graph fixing process and $t_n$ the threshold of the amount of graph common neighbors in this distance that is required to create a new edge. I suggest to choose $t_n = 0.75$ as a start parameter and $d = 0$, which deactivates the graph fixing. Please note that $d=0$ and $d=1$ are leading to the same results, because a distance of \num{1} is already given by the graph. If the results of GraBaSS contains to many similar subspaces with only a few different members, $d$ should be set to \num{2}. The choice of $t_n$ depends on the number of different subspace members. I suggest a value above \num{0.6}, because smaller values lead to a lose of to many information. If this still not help to improve the results, try to increase $d$ by \num{1}. If you get only a few but very big subspaces, $d$ is too high or $t_n$ is too low. Please note that applying the graph fixing always leads to a lose of information. In some cases, especially when manual data exploration is intended, it could be helpful to use it to get some more readable results.
