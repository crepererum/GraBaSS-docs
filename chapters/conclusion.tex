\chapter{Conclusion and further research}
Feature selection is an important task when analyzing data. It helps to understand relationships of attributes, speeds up further analysis tasks and handles the curse of dimensionality. Big data sets of today and the next years require an efficient way and parallel algorithms that provide solid results. This can only be done by using new approaches. Usability is another important aspect because it enables analysts to act faster and to get better results. With GraBaSS, I developed one small piece for the data processing of the next years. It is not only a algorithm, but also a framework and an illustration of ideas that can used by other researcher. It outperforms other approaches when in comes to calculation time and provides clearer, in some cases even better results. The implementation uses optimizations to archive high performance and low memory footprint and the developed data backend provides a foundation for other algorithms.

But GraBaSS and its implementation are not perfect. The chosen similarity is not suitable for all tasks, especially when special expectations are made, what means to be similar. It might be also possible to find a better common similarity which works well for the most data sets. The chosen post-filter is very simple. It may be replaced by other metrics e.g. for measuring contrast or filtering according to training data set so the result only contains subspaces where you can find special clusters or outliers very well. GraBaSS does not perform a pre-filtering so that dimensions get pruned before they are compared to others. This can speed up GraBaSS and avoids that the entire chain of feature selection and further processing operates on dimensions that do not contain useful information. Another part, which is not researched very well are incremental algorithms, which do not require a full recalculation when new data points or even new dimensions are added or even removed to the data set. This would enable better real time analysis of data, because incremental clustering would allow to choose $t_e$ interactively or by an automatic algorithm so it produces a good cluster count. Automatic parameter detection might also be possible when $t_n$ and $d$ should be chosen, because most humans are able to visually detect the problem of small quasi-cliques so an algorithm which does something similar could solve the problem for bigger data sets.

The realization in C++ is good but does not use GPUs or other accelerators like FGPAs and is not scalable about multiple computers. To archive this the first one, an OpenCL based implementation and further memory optimization could help. To use this common technology which operates as interface to GPUs, CPUs, FPGAs and other accelerators, the code has to refactored. It has to be split in small tasks and with a low amount of shared read-write memory. To scale over multiple computers it is required to provide a solid foundation of network code, error detection and load balancing. Heterogeneous architectures makes this task even more complicated. The data backend currently does not support any data checks or normalization for multiple architectures. A high performance, in-memory database which provides a generic interface for this kind of algorithm would be an ideal candidate to solve all these problems.

There are many topics left for research especially because data set size increases and real time information becomes more important in times where computers do not get faster but get more parallel computing units. I believe that this is not my last work in this field and I hope to provide a small piece for better technology that may help other people in a wide range of tasks, today and in future times.
