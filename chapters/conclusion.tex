\chapter{Conclusion and further research}
Feature selection is an important task when analyzing data. It helps to understand relationships of attributes, speeds up further analysis tasks and handles the curse of dimensionality. Big data sets of today and the next years require an efficient way and parallel algorithms that provide solid results. With GraBaSS, I developed one small piece for the data processing of the next years. It is not only a algorithm, but also a framework and an illustration of ideas that can used by other researcher. It outperforms other approaches when in comes to calculation time and provides clearer, in some cases even better results. The implementation uses optimizations to archive high performance and low memory footprint.

But GraBaSS and its implementation are not perfect. The chosen similarity is suitable for all tasks, especially when high contrast is required. The realization in C++ does not use GPUs or other accelerators like FGPAs and is not scalable about multiple computers. To archive this the first one, an OpenCL based implementation and further memory optimization could help. The latter one requires a solid foundation of network code, error detection and load balancing. Heterogeneous architectures makes this task even more complicated. A high performance, in-memory database which provides a generic interface for this kind of algorithm would be required. Another part, which is not researched very well are incremental algorithms, which do not require a full recalculation when new data points or even new dimensions are added to the data set.

So there are many topics left for research especially because data set size increases and real time information becomes more important. I believe that this is not my last work in this field and I hope to provide a small piece for better technology that may help other people in a wide range of tasks, today and in future times. 
