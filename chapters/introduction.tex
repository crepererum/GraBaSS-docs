\chapter{Introduction}\label{chap:introduction}
The 21th century is the age of information. Every year, the amount of new data that is accessible to analysts get higher. More sensor networks get installed, higher resolutions in booth, the data dimensions itself and the time get possible, more communication over the global network is made and new methods for measuring health, environment, social and finance parameters are developed. If this data is used by the right, ethical way, it can have a huge impact on our society, pushing the global development and making new technologies possible. 

In the last decades, many researchers developed methods to group data, predict new data or find anomalies. But more data does not only mean more data points, it also means more dimensions. And so, many methods getting slow or does not work anymore. This fact is known as curse of dimensionality. When the number of dimensions get higher, dimensions can be grouped together because they are similar or describe disjunct features. This process is also called feature selection and the groups can be called subspaces. If the data is projected into this subspaces, standard analysis methods can be used. So researchers developed methods for feature selection. They offer good results when used with high number of objects and a high number of dimensions and some of them are also proven in a theoretical way, but have one common problem: they are really slow. Having a cubic complexity in the number of dimensions, it's not possible to use them for todays or future data sets. Many of them also have no good for parallelized computing, which is highly important today and will get essential in the next years. Another problem are the parameters of the algorithms. If there are too many parameters, which interfere with each other and are not intuitive, analysts just choose default, random or dummy parameters or are not able to get a good result. It is also very common to have parameters which ranges that heavily depends on the structure of the input data and not only the data size. The perfect case would be a few parameters with intersected effects and fixed ranges.

So why is this a problem, you may ask. Just use a bigger computer or wait a day, a week, a month\ldots Because we are wasting the most important resource we have. It's not water, not energy, not oil, not gold or lithium. It's not knowledge or intelligence. Our most important resource is time and we all are running out of them. I believe that it is possible to be faster, to get information when you need it. Even when ad-hoc data analysis is not possible today, I believe it is possible in the future. And it will change everything, the way we consume information and media, the art of describing our environment and our society, the way people life and communicate, the methods of research, production, planning and design, even the way we think and decide.

As a first, small step to this future, I propose a new faster method for feature selection: Graph Based Subspace Search, or GraBaSS. It may not be as exact or mathematically proven as some of the competitors. But it works fast, parallel, the parameters are easy to choose and the results are intuitive. It is build on the insight, that most of the dimensions of a subspace are similar or not which leads to binary relationship. This relationship is enough to build a graph and find cliques that describes the subspaces in the most cases.
