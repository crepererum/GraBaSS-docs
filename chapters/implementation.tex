\chapter{Implementation}
For the researcher who want only get a described complexity it is sufficient to use a favorite programming languge or DBMS. But if you want to handle big data before getting to old, it is important to choose the right tools and think about some implementation details. In this chapter I want to descibe and explain my choices and would like to give you some impressions about the implementation process. Some but not all strategies can also be used for other algorithms and may be used for some other work too. I will finish the chapter with some ideas about future improvements.

\section{Choosing a programming language}
To express the designed algorithm and let the computer do the job it is necessary to write some source code. There are dozend different programming languages out there and every language has its own pros and cons.

A simple and very comfortable choice is a language with a good buildin runtime library.\footnote{What ``good'' means in this context depends on the algorithm that you want to implement and the operations you need.} Python or Matlab is a good choice. Please notice that SQL or other database bound languages are not very portable and suffer when it comes to debugging and flexibility. If the language provided a embedded mathematic system like Matlab, it is easy to map the theoretical terms to the language. Matrix support makes it easy to handle big amounts of data. The key feature of the language should be syntax that does not produce much boiler plate. This is one point where script languages are very good, but lanuages like Java are not. C++ provides a short syntax for many things, but suffers like many other compiled langues when you want a comfortable environment. Typesafety is another point that can be useful because it allowes you to tackle many bugs before you waste your time for runtime tests.

The next point is a good library support for things that the choosen language does not provide. Matrix operations require a special library, escecially if you want to invert or decomposit them. High precision floating point operations and big integer calculation needs a different library. Parsing, code managment or special IO operations is another topic. Also notice that I don't mean the standard library. A well tested and documented library is required. For example NumPy if you use Python.

In addition to the things you as the author and primary user of the implementation finds useful are the problems your team or the consumers need. An exotic language\footnote{Don't know what I mean? Go and find something about APL} may be very cool but if nobody can read your code, your research result doesn't help someone. This does not mean that you have to stuck in old good Java or COBOL, but it means that you have to think about the innovation you want to use. Also think about portability, future development and safety of the language. Safety is important if you don't want to risk the data of your institution or the consumers. You may think that the implementation is only for internal tests but if your results are good it can be the case that your code will be published or used by other persons. Even if low level languages can be insecure when doing unchecked memory operations, also interpreted languages have many weaknesses.

You may ask where this all have to do with high perfomance. The simple answer is: nothing. They make your life easier, but do not provide a fast implementation. So the most important point for my work is performance. And this kills the most high level languages, because they do not provide a fine grained control about data allocation, movement and copying. Even the object overhead of languages like Java and naiv C++ is too big. I had to strip down the calculation core to a simple low level core. But I do not want to loose all high level features. In my eyes, there is currently only one languages that provides this schizophrenia: handcrafted and well written C++. I will give you some rules about hints about the language specific methods in the next section.

\section{Special C++ tips}
C++ can be used in many different ways, depending on your background, the interfaces you want to use or have to provide and the attributes your program should have. As most other programming languages out there, C++ is avaiable in different standarts and the compiler and STL support differs. One of the best collections of tips for good programming style is \cite{effectiveCpp}. Furthermore I used the new C++11 standard which brings makes many code sections easier to read and more C++ stylish rather than C based. It also avoids many boiler blade an manual memory management. Templates may be the most hated but most also most loved and most essential features of C++. I used them when I find it useful. Some calculations are written in C style because in some cases, the object overhead was to high. Because of the usage of new C++11 featues, the resulting code only compiles with newer versions of GCC and Clang. Microsoft and Intel compilers are not suppported at the time of the writing.

\section{Avoiding reinventing the wheel}
The STL already provides useful tools that makes programmers life much easier. But it does not provide high level thread operations like fork and join, system operations like memory mapped IO and fast a fast parser library. C++ has a special collection of well designed and reviewed libraries that are note yet a part of the STL: The Boost C++ Libraries. They provide two of the three libraries I was looking for -- the Boost Interprocess library, which implements memory mapped IO and the Spirit V2 library which is a special set of function and templates for parser writing. For high level multi core operations I use another well known library: the TBB, which is designed and published by Intel. All libraries are open source and free which enables researcher to use them for their work.

\section{The low level data backend}
In the field of data processing and data mining, many algorithms where build on top of exisiting server driven databases. This has many benefits. In many cases, the data that should be analyzed already exists in this databases in a normalized form and is supplied by fresh data. Because of the nature of feature selection, column stores are than row oriented stores. They provide a cache local access to data of different dimensions and reduces the processing overhead. The main drawback of the most server driven data stores is the performance when using complex algorithms.\footnote{This may change in the next few years when in-memory databases become more prominent and affordable.} Another solution is using an embedded database like SQLite, but they also have too much overhead when accessing raw data. Please consider that the database also provides too many operations that are not needed for this kind of data processing. Only apped and read operations are required, no indices, no aggregations, no write, modify and delete.

This brings me to the question why I need a backend for data storage. In the current situation, most researchers don't have systems with enough main memory to hold the complete data. So it is important that they have a backend that can easely page out data to the disc or SSD. This should also be combined with low overhead. So sequencial loading on demand is not an option. Another problem is that the application programmer cannot easely determine, when the system runs out of memory, because the OS swaps out data on its own discretion. An old but very wise advice of OS designers is, that the kernel always has better information about the entire system than every user program. So why not let the kernel do the job? So I decided to just use plain arrays for column representation and memory map them from a file. So the kernel can load them or page them out if necessary. For better managment and better append operations, the columns are defided into fixed size segments. The segments and the column metadata like size and name are managed by the Boost Interprocess Library, which uses a trees to manage a string to pointer mapping.

As shown in chapter~\ref{chap:algodesign} the algorithm needs a graph structure. An append only graph can mapped to a column store by utilizing two columns and encode each vertex as an unique, continous, unsigned integer. One column is called the neighbor column and stores the appended list of all neighbors of all vertices. The other column is called the index column stores the split points of the neighbor column. To get back the neighbors of an vertex $v$, just output the neighbor column from the index that is written in the in index column at position $v$ to the index that is written at position $v+1$.

Maps for precalcuated metadata can also be mapped to column stores by storing key value tuples and rebuilding the hash index when loading the map. This does not have perfomance impact because the algorithm only need a small fixed number of metadata per dimension.

